{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week_04_HW_Transfer-Learning",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y4deLMAffDT",
        "colab_type": "text"
      },
      "source": [
        "# Detecting Pneumonia from X-rays\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> #### *Dataset from [Kaggle](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/data)*\n",
        "*https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/data*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## *Week 04 homework:*\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfilsI3GgEXM",
        "colab_type": "text"
      },
      "source": [
        "### *Importing libraries*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--0IAxsoF4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library imports\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import shutil\n",
        "import urllib.request\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, \\\n",
        "  GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, \\\n",
        "  preprocess_input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, \\\n",
        "  ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from itertools import product\n",
        "from functools import partial\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except:\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmM29w4fgMa-",
        "colab_type": "text"
      },
      "source": [
        "### *Mounting and Defining Filepaths*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlWm349lJRc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive to access the dataset from it\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtCpigfqJWVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define filepaths\n",
        "filepath = '/content/drive/My Drive/Week_04/'\n",
        "\n",
        "train_dir =  filepath + 'train/'\n",
        "validation_dir = filepath + 'val/'\n",
        "test_dir = filepath + 'test/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INQ_aFbTgVwJ",
        "colab_type": "text"
      },
      "source": [
        "### *Building the model with custom layers on top*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll9McTKsJf5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "b7370bf7-029f-4c04-e31f-e169446e17df"
      },
      "source": [
        "# Number of classes to predict (Normal or Pneumonia(2))\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Build Model\n",
        "def create_model(input_shape, num_classes):\n",
        "\n",
        "  ## Create and return tensorflow model (building ontop of InceptionV3).  \n",
        "\n",
        "  # Blank slate\n",
        "  K.clear_session()\n",
        "\n",
        "  # Inception v3 for base model\n",
        "  base_model = InceptionV3(weights='imagenet', include_top=False,\\\n",
        "                           input_shape=input_shape)\n",
        "  \n",
        "  # Add 3 custom layers on top\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "  x = Dense(512, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  \n",
        "  # Using base model for feature extraction. layer weights don't change\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Final prediction layer is dense\n",
        "  predictions = Dense(num_classes, activation='sigmoid')(x)\n",
        "  \n",
        "  # Instantiate model \n",
        "  model = Model(inputs=base_model.inputs, outputs=predictions)\n",
        "\n",
        "  return model\n",
        "\n",
        "# Height & Width is 150 and 3 channels for Inception\n",
        "model = create_model((150, 150, 3), NUM_CLASSES)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsQ0LgbfhMWg",
        "colab_type": "text"
      },
      "source": [
        "### *Define training & testing accuracy / loss*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyv9G_wZJgc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Loss\n",
        "training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "\n",
        "# Training Accuracy\n",
        "training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    'training_accuracy', dtype=tf.float32)\n",
        "\n",
        "# Test Loss \n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "\n",
        "# Test Accuracy \n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    'test_accuracy', dtype=tf.float32)\n",
        "\n",
        "# Print model summary \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUxexVY5hZhx",
        "colab_type": "text"
      },
      "source": [
        "### *Define optimizer and compile the model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1twRooAEJiYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam Optimizer \n",
        "optimizer = Adam(lr=0.0001)\n",
        "\n",
        "# Compile \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWiSQiHPhe1d",
        "colab_type": "text"
      },
      "source": [
        "### *File count function*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYLkGIJ6JlYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dir_file_count(directory):\n",
        "\n",
        "  # Number of files present inside the 'directory'.  \n",
        "  return sum([len(files) for r, d, files in os.walk(directory)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1XBnS5gkmpm",
        "colab_type": "text"
      },
      "source": [
        "### *Configure paramaters, setup generators*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A32WjrfJnOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c3fee5ad-fa89-4a8e-9cbe-e18f67c3c29c"
      },
      "source": [
        "# Config parameters \n",
        "rescale = 1./255\n",
        "target_size = (150, 150)\n",
        "batch_size = 500\n",
        "class_mode = 'categorical'\n",
        "\n",
        "# Augment the Training dataset images \n",
        "train_datagen = ImageDataGenerator(rescale=rescale,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)\n",
        "# Load the images in the generator \n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    target_size=target_size,\n",
        "                                                    class_mode=class_mode,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True)\n",
        "# Augment the validation dataset images\n",
        "val_datagen = ImageDataGenerator(rescale=rescale)\n",
        "\n",
        "# Load the images in the generator\n",
        "val_generator = val_datagen.flow_from_directory(validation_dir, \n",
        "                                                target_size=target_size,\n",
        "                                                class_mode=class_mode,\n",
        "                                                batch_size=dir_file_count(validation_dir),\n",
        "                                                shuffle=False)\n",
        "# Augment the test dataset images\n",
        "test_datagen = ImageDataGenerator(rescale=rescale)\n",
        "\n",
        "# Load the images in the generator\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=target_size,\n",
        "                                                  class_mode=class_mode,\n",
        "                                                  batch_size=dir_file_count(test_dir),\n",
        "                                                  shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5241 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfMX7W8DkyKc",
        "colab_type": "text"
      },
      "source": [
        "### *Weights*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDToWV1_JqYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a05730f2-793c-41dc-de0a-4d2d14bd784e"
      },
      "source": [
        "y = train_generator.classes\n",
        "labels = np.unique(y)\n",
        "\n",
        "train_class_weights = compute_class_weight('balanced', labels, y)\n",
        "print(train_class_weights)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.94111111 0.67347726]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrT5YW6lk2CY",
        "colab_type": "text"
      },
      "source": [
        "### *Start training and save model file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSPQ2ZgYJqzk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "c8722fba-215a-40f8-d1ab-09e407c814a9"
      },
      "source": [
        "# Start the training\n",
        "history = model.fit_generator(train_generator,\n",
        "                              steps_per_epoch=len(train_generator),\n",
        "                              epochs=10,\n",
        "                              verbose=1,\n",
        "                              validation_data=val_generator,\n",
        "                              validation_steps=len(val_generator),\n",
        "                              class_weight=train_class_weights,\n",
        "                              workers=20)\n",
        "\n",
        "# Save the model after the training is complete\n",
        "MODEL_FILE = 'pneumonia_v1.hd5'\n",
        "model.save(MODEL_FILE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "11/11 [==============================] - 526s 48s/step - loss: 0.6469 - acc: 0.6743 - val_loss: 0.6419 - val_acc: 0.5000\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 366s 33s/step - loss: 0.5539 - acc: 0.7539 - val_loss: 0.6149 - val_acc: 0.6250\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 352s 32s/step - loss: 0.4804 - acc: 0.7787 - val_loss: 0.7281 - val_acc: 0.5625\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 361s 33s/step - loss: 0.4158 - acc: 0.8342 - val_loss: 0.9349 - val_acc: 0.6250\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 353s 32s/step - loss: 0.3498 - acc: 0.8544 - val_loss: 1.6428 - val_acc: 0.5625\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 361s 33s/step - loss: 0.3299 - acc: 0.8693 - val_loss: 1.9249 - val_acc: 0.6250\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 352s 32s/step - loss: 0.3126 - acc: 0.8823 - val_loss: 1.6182 - val_acc: 0.6250\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 353s 32s/step - loss: 0.2873 - acc: 0.8844 - val_loss: 1.9508 - val_acc: 0.5000\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 361s 33s/step - loss: 0.2678 - acc: 0.8945 - val_loss: 1.3165 - val_acc: 0.6875\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 353s 32s/step - loss: 0.2706 - acc: 0.8901 - val_loss: 1.3608 - val_acc: 0.6875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWGcc4brlF1j",
        "colab_type": "text"
      },
      "source": [
        "### *Move model file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0RebA4qJsl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move Model file to Google Drive \n",
        "!mv {MODEL_FILE} '/content/drive/My Drive/Week_04'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG2iXzKElJRW",
        "colab_type": "text"
      },
      "source": [
        "### *Load model file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFTb3XzzJu81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Model File from Google Drive \n",
        "MODEL_FILE = f'/content/drive/My Drive/Week_04/{MODEL_FILE}'\n",
        "model = tf.keras.models.load_model(MODEL_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJt5ru36lM46",
        "colab_type": "text"
      },
      "source": [
        "### *Evaluate test generator*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "620jlzTKJxI9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a13ef61b-06e4-4996-ce91-b58711466b0f"
      },
      "source": [
        "# Evaluate test generator\n",
        "result = model.evaluate_generator(test_generator, steps=len(test_generator),\\\n",
        "                                  verbose=1)\n",
        "\n",
        "print(\"%s%.2f  \"% (\"Loss     : \", result[0]))\n",
        "print(\"%s%.2f%s\"% (\"Accuracy : \", result[1]*100, \"%\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 207s 207s/step - loss: 1.3821 - acc: 0.6202\n",
            "Loss     : 1.38  \n",
            "Accuracy : 62.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU_xbmDXlYQy",
        "colab_type": "text"
      },
      "source": [
        "### *Results / predict*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW1iWluWJy6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "184836fb-ff26-44d3-9b06-6e4c11942c96"
      },
      "source": [
        "# Randomly generate Test Batch\n",
        "num_of_batch = len(test_generator) # This is 1 in our case \n",
        "batch_no = random.randint(0, num_of_batch - 1)\n",
        "\n",
        "# Fetch batch data\n",
        "y_img_batch, y_true_batch = test_generator.__getitem__(batch_no)\n",
        "y_true_batch = y_true_batch.argmax(axis=-1)\n",
        "\n",
        "# Make predictions \n",
        "y_pred_batch = model.predict(y_img_batch)\n",
        "y_pred_batch = y_pred_batch.argmax(axis=-1)\n",
        "\n",
        "# Print results \n",
        "print(\"-\"*35)\n",
        "print(\"%s%d\"%     (\"Selected Batch No       : \", batch_no))\n",
        "print(\"-\"*35)\n",
        "print(\"%s%d\"%     (\"Batch Size              : \", len(y_pred_batch)))\n",
        "print(\"-\"*35)\n",
        "print(\"%s%.2f%s\"% (\"Accuracy                : \", np.mean(y_true_batch==y_pred_batch)*100, \"%\"))\n",
        "print(\"-\"*35)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------\n",
            "Selected Batch No       : 0\n",
            "-----------------------------------\n",
            "Batch Size              : 624\n",
            "-----------------------------------\n",
            "Accuracy                : 62.02%\n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}